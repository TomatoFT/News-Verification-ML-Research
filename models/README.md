# Models Spaces

This directory is the places for storing some of the model

## Models

The following NLP models are available in this repository:

1. **AlBERT**
   - A lightweight and efficient version of BERT designed for fast training and deployment.

2. **BERT**
   - Bidirectional Encoder Representations from Transformers (BERT) is a pre-trained transformer model that captures contextual information from both left and right contexts.

3. **CamemBERT**
   - A French language model based on BERT, tailored for natural language understanding tasks in French.

4. **DistilBERT**
   - A distilled version of BERT, offering a smaller and faster model while retaining much of the performance.

5. **RetriBERT**
   - A French language model inspired by CamemBERT, designed for French NLP applications.

6. **MobileBERT**
   - An optimized and compressed version of BERT, suitable for mobile and edge devices.

7. **PhoBERT**
   - A pre-trained model for Vietnamese, based on BERT, tailored for Vietnamese NLP tasks.

8. **RoBERTa**
   - A robustly optimized BERT approach that removes the next sentence prediction objective and introduces dynamic masking.

9. **XLMNet**
   - A transformer model trained on multiple languages using the Cross-lingual Language Model (XLM) approach, promoting multilingual capabilities.

